{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date_ordered   product  quantity        price                   week\n",
      "0     2023-01-01    Hybrid        12   551.552635  2022-12-26/2023-01-01\n",
      "1     2023-01-02  Mountain        18   533.982983  2023-01-02/2023-01-08\n",
      "2     2023-01-03    Hybrid        28   642.018731  2023-01-02/2023-01-08\n",
      "3     2023-01-04    Hybrid        22   692.178061  2023-01-02/2023-01-08\n",
      "4     2023-01-05  Mountain        19   539.013758  2023-01-02/2023-01-08\n",
      "..           ...       ...       ...          ...                    ...\n",
      "360   2023-12-27      Road        21   856.929812  2023-12-25/2023-12-31\n",
      "361   2023-12-28    Hybrid        13   563.371609  2023-12-25/2023-12-31\n",
      "362   2023-12-29      Road         5  1020.860955  2023-12-25/2023-12-31\n",
      "363   2023-12-30      Road        25   782.745158  2023-12-25/2023-12-31\n",
      "364   2023-12-31    Hybrid        24   628.987756  2023-12-25/2023-12-31\n",
      "\n",
      "[365 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating a sample dataset\n",
    "np.random.seed(42)\n",
    "start_date = datetime(2023, 1, 1)\n",
    "dates = [start_date + timedelta(days=i) for i in range(365)]  # One year of data\n",
    "products = np.random.choice(['Mountain', 'Road', 'Hybrid'], size=365)\n",
    "quantities = np.random.randint(5, 30, size=365)  # Daily sales\n",
    "prices = np.random.normal(1, 0.1, size=365) * np.where(products == 'Mountain', 500, np.where(products == 'Road', 800, 650))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date_ordered': dates,\n",
    "    'product': products,\n",
    "    'quantity': quantities,\n",
    "    'price': prices\n",
    "})\n",
    "\n",
    "# Aggregate data to weekly sales\n",
    "df['week'] = df['date_ordered'].dt.to_period('W')\n",
    "weekly_sales = df.groupby(['week', 'product']).agg({'quantity': 'sum', 'price': 'mean'}).reset_index()\n",
    "weekly_sales['date_ordered'] = weekly_sales['week'].dt.start_time\n",
    "weekly_sales = weekly_sales.drop('week', axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 0.5811 - mae: 0.5127 - val_loss: 0.4147 - val_mae: 0.4660\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5531 - mae: 0.4993 - val_loss: 0.4048 - val_mae: 0.4694\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5866 - mae: 0.5236 - val_loss: 0.3956 - val_mae: 0.4731\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5599 - mae: 0.5183 - val_loss: 0.3859 - val_mae: 0.4770\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5486 - mae: 0.5069 - val_loss: 0.3760 - val_mae: 0.4814\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5374 - mae: 0.5129 - val_loss: 0.3655 - val_mae: 0.4863\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5186 - mae: 0.5316 - val_loss: 0.3550 - val_mae: 0.4918\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4995 - mae: 0.5234 - val_loss: 0.3464 - val_mae: 0.4971\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5078 - mae: 0.5449 - val_loss: 0.3386 - val_mae: 0.5031\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4931 - mae: 0.5461 - val_loss: 0.3326 - val_mae: 0.5098\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5181 - mae: 0.5590 - val_loss: 0.3297 - val_mae: 0.5151\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4838 - mae: 0.5602 - val_loss: 0.3278 - val_mae: 0.5168\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4865 - mae: 0.5648 - val_loss: 0.3259 - val_mae: 0.5148\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5193 - mae: 0.5734 - val_loss: 0.3247 - val_mae: 0.5118\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4913 - mae: 0.5625 - val_loss: 0.3238 - val_mae: 0.5101\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5519 - mae: 0.5831 - val_loss: 0.3228 - val_mae: 0.5095\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5146 - mae: 0.5686 - val_loss: 0.3216 - val_mae: 0.5096\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5324 - mae: 0.5704 - val_loss: 0.3209 - val_mae: 0.5082\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4773 - mae: 0.5533 - val_loss: 0.3207 - val_mae: 0.5068\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5221 - mae: 0.5701 - val_loss: 0.3200 - val_mae: 0.5058\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4729 - mae: 0.5511 - val_loss: 0.3180 - val_mae: 0.5059\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4870 - mae: 0.5655 - val_loss: 0.3165 - val_mae: 0.5053\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4806 - mae: 0.5572 - val_loss: 0.3152 - val_mae: 0.5040\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5154 - mae: 0.5718 - val_loss: 0.3143 - val_mae: 0.5024\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4755 - mae: 0.5571 - val_loss: 0.3133 - val_mae: 0.5015\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4919 - mae: 0.5657 - val_loss: 0.3118 - val_mae: 0.5016\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4827 - mae: 0.5567 - val_loss: 0.3122 - val_mae: 0.5009\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5054 - mae: 0.5678 - val_loss: 0.3137 - val_mae: 0.5002\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4668 - mae: 0.5427 - val_loss: 0.3152 - val_mae: 0.5004\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4774 - mae: 0.5433 - val_loss: 0.3132 - val_mae: 0.5021\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4853 - mae: 0.5455 - val_loss: 0.3097 - val_mae: 0.5026\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4679 - mae: 0.5536 - val_loss: 0.3067 - val_mae: 0.5011\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4307 - mae: 0.5327 - val_loss: 0.3037 - val_mae: 0.4951\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5137 - mae: 0.5628 - val_loss: 0.3020 - val_mae: 0.4901\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4669 - mae: 0.5409 - val_loss: 0.3005 - val_mae: 0.4868\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4434 - mae: 0.5319 - val_loss: 0.3003 - val_mae: 0.4889\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4686 - mae: 0.5386 - val_loss: 0.3056 - val_mae: 0.4932\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4840 - mae: 0.5404 - val_loss: 0.3086 - val_mae: 0.4933\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4789 - mae: 0.5396 - val_loss: 0.3068 - val_mae: 0.4884\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4134 - mae: 0.5043 - val_loss: 0.2973 - val_mae: 0.4750\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4153 - mae: 0.5026 - val_loss: 0.3029 - val_mae: 0.4641\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4693 - mae: 0.5132 - val_loss: 0.3105 - val_mae: 0.4625\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4314 - mae: 0.4961 - val_loss: 0.3034 - val_mae: 0.4669\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4443 - mae: 0.4988 - val_loss: 0.3089 - val_mae: 0.4732\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4259 - mae: 0.4938 - val_loss: 0.3113 - val_mae: 0.4753\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4186 - mae: 0.4841 - val_loss: 0.3239 - val_mae: 0.4763\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4035 - mae: 0.4829 - val_loss: 0.3372 - val_mae: 0.4729\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4219 - mae: 0.4690 - val_loss: 0.3197 - val_mae: 0.4559\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3672 - mae: 0.4471 - val_loss: 0.3171 - val_mae: 0.4457\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3904 - mae: 0.4470 - val_loss: 0.3300 - val_mae: 0.4483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4261 - mae: 0.4543\n",
      "Test Loss: 0.4261409044265747, Test MAE: 0.4542675316333771\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # For newer versions of scikit-learn\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(sparse_output=False), ['product']),\n",
    "        ('scaler', StandardScaler(), ['quantity', 'price'])\n",
    "    ])\n",
    "except TypeError:\n",
    "    # For older versions of scikit-learn\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(sparse=False), ['product']),\n",
    "        ('scaler', StandardScaler(), ['quantity', 'price'])\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "data = column_transformer.fit_transform(weekly_sales)\n",
    "\n",
    "n_features = data.shape[1]\n",
    "\n",
    "# Create sequences (use last 4 weeks to predict the next week)\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        end_ix = i + n_steps\n",
    "        seq_x = data[i:end_ix]\n",
    "        seq_y = data[end_ix, 1:4]  # Predict quantities for all 3 products\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 4\n",
    "X, y = create_sequences(data, n_steps)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(n_steps, n_features)),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dense(3)  # Output layer with 3 units (one for each product)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Model evaluation\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (29, 4, 5)\n",
      "Shape of predictions: (29, 3)\n",
      "Shape of product_inverse: (29, 1)\n",
      "Error in inverse_transform: operands could not be broadcast together with shapes (29,3) (2,) (29,3) \n",
      "Shape of scaler.scale_: (2,)\n",
      "Shape of scaler.mean_: (2,)\n",
      "Debug info: {'X_test_shape': (29, 4, 5), 'predictions_shape': (29, 3), 'product_inverse_shape': (29, 1)}\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform_predictions(column_transformer, X_test, predictions):\n",
    "    # Get the OneHotEncoder and StandardScaler from the ColumnTransformer\n",
    "    ohe = column_transformer.named_transformers_['ohe']\n",
    "    scaler = column_transformer.named_transformers_['scaler']\n",
    "    \n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    \n",
    "    # Inverse transform the one-hot encoded product\n",
    "    product_inverse = ohe.inverse_transform(X_test[:, -1, :3])\n",
    "    print(f\"Shape of product_inverse: {product_inverse.shape}\")\n",
    "    \n",
    "    # Inverse transform the scaled quantities\n",
    "    try:\n",
    "        quantities_inverse = scaler.inverse_transform(predictions)\n",
    "        print(f\"Shape of quantities_inverse: {quantities_inverse.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in inverse_transform: {e}\")\n",
    "        print(f\"Shape of scaler.scale_: {scaler.scale_.shape}\")\n",
    "        print(f\"Shape of scaler.mean_: {scaler.mean_.shape}\")\n",
    "    \n",
    "    # For debugging, return the shapes\n",
    "    return {\n",
    "        'X_test_shape': X_test.shape,\n",
    "        'predictions_shape': predictions.shape,\n",
    "        'product_inverse_shape': product_inverse.shape,\n",
    "    }\n",
    "\n",
    "# Use the updated function\n",
    "debug_info = inverse_transform_predictions(column_transformer, X_test, predictions)\n",
    "print(\"Debug info:\", debug_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions (date, product, quantity):\n",
      "2023-01-01: Hybrid: 54\n",
      "2023-01-08: Hybrid: 52\n",
      "2023-01-15: Mountain: 43\n",
      "2023-01-22: Road: 57\n",
      "2023-01-29: Hybrid: 56\n",
      "2023-02-05: Hybrid: 56\n",
      "2023-02-12: Road: 50\n",
      "2023-02-19: Hybrid: 54\n",
      "2023-02-26: Hybrid: 52\n",
      "2023-03-05: Mountain: 52\n",
      "2023-03-12: Hybrid: 52\n",
      "2023-03-19: Road: 53\n",
      "2023-03-26: Hybrid: 54\n",
      "2023-04-02: Road: 46\n",
      "2023-04-09: Hybrid: 51\n",
      "\n",
      "Shape of predictions_original_scale: (29, 3)\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform_predictions(column_transformer, X_test, predictions, dates):\n",
    "    # Get the OneHotEncoder and StandardScaler from the ColumnTransformer\n",
    "    ohe = column_transformer.named_transformers_['ohe']\n",
    "    scaler = column_transformer.named_transformers_['scaler']\n",
    "    \n",
    "    # Inverse transform the one-hot encoded product\n",
    "    product_inverse = ohe.inverse_transform(X_test[:, -1, :3])\n",
    "    \n",
    "    # Inverse transform the scaled quantities\n",
    "    quantities_inverse = predictions * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    # Combine the results\n",
    "    result = []\n",
    "    for date, prods, quants in zip(dates, product_inverse, quantities_inverse):\n",
    "        result.extend((date, prod, quant) for prod, quant in zip(prods, quants))\n",
    "    \n",
    "    return np.array(result, dtype=object)\n",
    "\n",
    "# Assuming you have a dates array corresponding to your predictions\n",
    "# If not, you'll need to create this based on your data\n",
    "dates = pd.date_range(start='2023-01-01', periods=len(predictions), freq='W')\n",
    "\n",
    "# Use the updated function\n",
    "predictions_original_scale = inverse_transform_predictions(column_transformer, X_test, predictions, dates)\n",
    "\n",
    "print(\"Sample predictions (date, product, quantity):\")\n",
    "for date, product, quantity in predictions_original_scale[:15]:\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: {product}: {float(quantity):.0f}\")\n",
    "\n",
    "# Additional debugging information\n",
    "print(\"\\nShape of predictions_original_scale:\", predictions_original_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions (date, product, quantity, potential influence):\n",
      "2023-01-01: Hybrid: 54 - Christmas/New Year\n",
      "2023-01-08: Hybrid: 52 - No major influence\n",
      "2023-01-15: Mountain: 43 - No major influence\n",
      "2023-01-22: Road: 57 - No major influence\n",
      "2023-01-29: Hybrid: 56 - No major influence\n",
      "2023-02-05: Hybrid: 56 - No major influence\n",
      "2023-02-12: Road: 50 - No major influence\n",
      "2023-02-19: Hybrid: 54 - No major influence\n",
      "2023-02-26: Hybrid: 52 - No major influence\n",
      "2023-03-05: Mountain: 52 - Spring Season\n",
      "2023-03-12: Hybrid: 52 - Spring Season\n",
      "2023-03-19: Road: 53 - Spring Season\n",
      "2023-03-26: Hybrid: 54 - Spring Season\n",
      "2023-04-02: Road: 46 - Spring Season\n",
      "2023-04-09: Hybrid: 51 - Spring Season\n",
      "\n",
      "Shape of predictions_original_scale: (29, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def get_potential_influence(date):\n",
    "    \"\"\"Determine potential influences based on the date.\"\"\"\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    if month == 12 or (month == 1 and day <= 7):\n",
    "        return \"Christmas/New Year\"\n",
    "    elif month == 11 and day >= 20:\n",
    "        return \"Black Friday/Cyber Monday\"\n",
    "    elif 3 <= month <= 5:\n",
    "        return \"Spring Season\"\n",
    "    elif 6 <= month <= 8:\n",
    "        return \"Summer Season\"\n",
    "    elif month == 9:\n",
    "        return \"Back to School\"\n",
    "    else:\n",
    "        return \"No major influence\"\n",
    "\n",
    "def inverse_transform_predictions(column_transformer, X_test, predictions, dates):\n",
    "    # Get the OneHotEncoder and StandardScaler from the ColumnTransformer\n",
    "    ohe = column_transformer.named_transformers_['ohe']\n",
    "    scaler = column_transformer.named_transformers_['scaler']\n",
    "    \n",
    "    # Inverse transform the one-hot encoded product\n",
    "    product_inverse = ohe.inverse_transform(X_test[:, -1, :3])\n",
    "    \n",
    "    # Inverse transform the scaled quantities\n",
    "    quantities_inverse = predictions * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    # Combine the results\n",
    "    result = []\n",
    "    for date, prods, quants in zip(dates, product_inverse, quantities_inverse):\n",
    "        influence = get_potential_influence(date)\n",
    "        result.extend((date, prod, quant, influence) for prod, quant in zip(prods, quants))\n",
    "    \n",
    "    return np.array(result, dtype=object)\n",
    "\n",
    "# Assuming you have a dates array corresponding to your predictions\n",
    "# If not, you'll need to create this based on your data\n",
    "dates = pd.date_range(start='2023-01-01', periods=len(predictions), freq='W')\n",
    "\n",
    "# Use the updated function\n",
    "predictions_original_scale = inverse_transform_predictions(column_transformer, X_test, predictions, dates)\n",
    "\n",
    "print(\"Sample predictions (date, product, quantity, potential influence):\")\n",
    "for date, product, quantity, influence in predictions_original_scale[:15]:\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: {product}: {float(quantity):.0f} - {influence}\")\n",
    "\n",
    "# Additional debugging information\n",
    "print(\"\\nShape of predictions_original_scale:\", predictions_original_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
