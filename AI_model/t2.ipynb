{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date_ordered   product  quantity        price                   week\n",
      "0     2023-01-01    Hybrid        12   551.552635  2022-12-26/2023-01-01\n",
      "1     2023-01-02  Mountain        18   533.982983  2023-01-02/2023-01-08\n",
      "2     2023-01-03    Hybrid        28   642.018731  2023-01-02/2023-01-08\n",
      "3     2023-01-04    Hybrid        22   692.178061  2023-01-02/2023-01-08\n",
      "4     2023-01-05  Mountain        19   539.013758  2023-01-02/2023-01-08\n",
      "..           ...       ...       ...          ...                    ...\n",
      "360   2023-12-27      Road        21   856.929812  2023-12-25/2023-12-31\n",
      "361   2023-12-28    Hybrid        13   563.371609  2023-12-25/2023-12-31\n",
      "362   2023-12-29      Road         5  1020.860955  2023-12-25/2023-12-31\n",
      "363   2023-12-30      Road        25   782.745158  2023-12-25/2023-12-31\n",
      "364   2023-12-31    Hybrid        24   628.987756  2023-12-25/2023-12-31\n",
      "\n",
      "[365 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating a sample dataset\n",
    "np.random.seed(42)\n",
    "start_date = datetime(2023, 1, 1)\n",
    "dates = [start_date + timedelta(days=i) for i in range(365)]  # One year of data\n",
    "products = np.random.choice(['Mountain', 'Road', 'Hybrid'], size=365)\n",
    "quantities = np.random.randint(5, 30, size=365)  # Daily sales\n",
    "prices = np.random.normal(1, 0.1, size=365) * np.where(products == 'Mountain', 500, np.where(products == 'Road', 800, 650))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date_ordered': dates,\n",
    "    'product': products,\n",
    "    'quantity': quantities,\n",
    "    'price': prices\n",
    "})\n",
    "\n",
    "# Aggregate data to weekly sales\n",
    "df['week'] = df['date_ordered'].dt.to_period('W')\n",
    "weekly_sales = df.groupby(['week', 'product']).agg({'quantity': 'sum', 'price': 'mean'}).reset_index()\n",
    "weekly_sales['date_ordered'] = weekly_sales['week'].dt.start_time\n",
    "weekly_sales = weekly_sales.drop('week', axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 384ms/step - loss: 0.5722 - mae: 0.4961 - val_loss: 0.4082 - val_mae: 0.4688\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5501 - mae: 0.5005 - val_loss: 0.3946 - val_mae: 0.4741\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5426 - mae: 0.5092 - val_loss: 0.3822 - val_mae: 0.4793\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4957 - mae: 0.5068 - val_loss: 0.3702 - val_mae: 0.4854\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5486 - mae: 0.5299 - val_loss: 0.3592 - val_mae: 0.4922\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5121 - mae: 0.5368 - val_loss: 0.3485 - val_mae: 0.5007\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5148 - mae: 0.5504 - val_loss: 0.3395 - val_mae: 0.5107\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4558 - mae: 0.5330 - val_loss: 0.3342 - val_mae: 0.5218\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5566 - mae: 0.5935 - val_loss: 0.3324 - val_mae: 0.5293\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4953 - mae: 0.5758 - val_loss: 0.3315 - val_mae: 0.5288\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4932 - mae: 0.5769 - val_loss: 0.3304 - val_mae: 0.5247\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4881 - mae: 0.5672 - val_loss: 0.3302 - val_mae: 0.5206\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4660 - mae: 0.5539 - val_loss: 0.3304 - val_mae: 0.5183\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5151 - mae: 0.5729 - val_loss: 0.3303 - val_mae: 0.5165\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4858 - mae: 0.5608 - val_loss: 0.3297 - val_mae: 0.5154\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4797 - mae: 0.5541 - val_loss: 0.3285 - val_mae: 0.5123\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4812 - mae: 0.5508 - val_loss: 0.3275 - val_mae: 0.5100\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4635 - mae: 0.5458 - val_loss: 0.3257 - val_mae: 0.5098\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5047 - mae: 0.5602 - val_loss: 0.3238 - val_mae: 0.5102\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4755 - mae: 0.5534 - val_loss: 0.3217 - val_mae: 0.5112\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4978 - mae: 0.5580 - val_loss: 0.3198 - val_mae: 0.5113\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4778 - mae: 0.5543 - val_loss: 0.3175 - val_mae: 0.5125\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4961 - mae: 0.5724 - val_loss: 0.3155 - val_mae: 0.5117\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4983 - mae: 0.5704 - val_loss: 0.3145 - val_mae: 0.5089\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4280 - mae: 0.5328 - val_loss: 0.3139 - val_mae: 0.5061\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5036 - mae: 0.5622 - val_loss: 0.3132 - val_mae: 0.5050\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5006 - mae: 0.5539 - val_loss: 0.3121 - val_mae: 0.5037\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4876 - mae: 0.5502 - val_loss: 0.3117 - val_mae: 0.5035\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4773 - mae: 0.5506 - val_loss: 0.3105 - val_mae: 0.5022\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4526 - mae: 0.5358 - val_loss: 0.3090 - val_mae: 0.4998\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4546 - mae: 0.5353 - val_loss: 0.3087 - val_mae: 0.4962\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4507 - mae: 0.5196 - val_loss: 0.3069 - val_mae: 0.4905\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4387 - mae: 0.5111 - val_loss: 0.3056 - val_mae: 0.4881\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4595 - mae: 0.5179 - val_loss: 0.3018 - val_mae: 0.4804\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4733 - mae: 0.5136 - val_loss: 0.2996 - val_mae: 0.4743\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4434 - mae: 0.5008 - val_loss: 0.2974 - val_mae: 0.4637\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4251 - mae: 0.4848 - val_loss: 0.2904 - val_mae: 0.4496\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4070 - mae: 0.4739 - val_loss: 0.2929 - val_mae: 0.4459\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4433 - mae: 0.4808 - val_loss: 0.2837 - val_mae: 0.4366\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3944 - mae: 0.4460 - val_loss: 0.2819 - val_mae: 0.4294\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4074 - mae: 0.4506 - val_loss: 0.2828 - val_mae: 0.4263\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3981 - mae: 0.4444 - val_loss: 0.2937 - val_mae: 0.4300\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4237 - mae: 0.4582 - val_loss: 0.2783 - val_mae: 0.4194\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3822 - mae: 0.4313 - val_loss: 0.2799 - val_mae: 0.4080\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3939 - mae: 0.4316 - val_loss: 0.2792 - val_mae: 0.3976\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3870 - mae: 0.4230 - val_loss: 0.2734 - val_mae: 0.3914\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4135 - mae: 0.4288 - val_loss: 0.2916 - val_mae: 0.4095\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3809 - mae: 0.4304 - val_loss: 0.2810 - val_mae: 0.3924\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3477 - mae: 0.4008 - val_loss: 0.2676 - val_mae: 0.3667\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3048 - mae: 0.3769 - val_loss: 0.2674 - val_mae: 0.3690\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.3588 - mae: 0.4014\n",
      "Test Loss: 0.3587515652179718, Test MAE: 0.4014478921890259\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002744D1B7600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # For newer versions of scikit-learn\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(sparse_output=False), ['product']),\n",
    "        ('scaler', StandardScaler(), ['quantity', 'price'])\n",
    "    ])\n",
    "except TypeError:\n",
    "    # For older versions of scikit-learn\n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(sparse=False), ['product']),\n",
    "        ('scaler', StandardScaler(), ['quantity', 'price'])\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "data = column_transformer.fit_transform(weekly_sales)\n",
    "\n",
    "n_features = data.shape[1]\n",
    "\n",
    "# Create sequences (use last 4 weeks to predict the next week)\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        end_ix = i + n_steps\n",
    "        seq_x = data[i:end_ix]\n",
    "        seq_y = data[end_ix, 1:4]  # Predict quantities for all 3 products\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 4\n",
    "X, y = create_sequences(data, n_steps)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(n_steps, n_features)),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dense(3)  # Output layer with 3 units (one for each product)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Model evaluation\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (29, 4, 5)\n",
      "Shape of predictions: (29, 3)\n",
      "Shape of product_inverse: (29, 1)\n",
      "Error in inverse_transform: operands could not be broadcast together with shapes (29,3) (2,) (29,3) \n",
      "Shape of scaler.scale_: (2,)\n",
      "Shape of scaler.mean_: (2,)\n",
      "Debug info: {'X_test_shape': (29, 4, 5), 'predictions_shape': (29, 3), 'product_inverse_shape': (29, 1)}\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform_predictions(column_transformer, X_test, predictions):\n",
    "    # Get the OneHotEncoder and StandardScaler from the ColumnTransformer\n",
    "    ohe = column_transformer.named_transformers_['ohe']\n",
    "    scaler = column_transformer.named_transformers_['scaler']\n",
    "    \n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    \n",
    "    # Inverse transform the one-hot encoded product\n",
    "    product_inverse = ohe.inverse_transform(X_test[:, -1, :3])\n",
    "    print(f\"Shape of product_inverse: {product_inverse.shape}\")\n",
    "    \n",
    "    # Inverse transform the scaled quantities\n",
    "    try:\n",
    "        quantities_inverse = scaler.inverse_transform(predictions)\n",
    "        print(f\"Shape of quantities_inverse: {quantities_inverse.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in inverse_transform: {e}\")\n",
    "        print(f\"Shape of scaler.scale_: {scaler.scale_.shape}\")\n",
    "        print(f\"Shape of scaler.mean_: {scaler.mean_.shape}\")\n",
    "    \n",
    "    # For debugging, return the shapes\n",
    "    return {\n",
    "        'X_test_shape': X_test.shape,\n",
    "        'predictions_shape': predictions.shape,\n",
    "        'product_inverse_shape': product_inverse.shape,\n",
    "    }\n",
    "\n",
    "# Use the updated function\n",
    "debug_info = inverse_transform_predictions(column_transformer, X_test, predictions)\n",
    "print(\"Debug info:\", debug_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions (date, product, quantity):\n",
      "2023-01-01: Hybrid: 58\n",
      "2023-01-08: Hybrid: 51\n",
      "2023-01-15: Mountain: 39\n",
      "2023-01-22: Road: 55\n",
      "2023-01-29: Hybrid: 53\n",
      "2023-02-05: Hybrid: 60\n",
      "2023-02-12: Road: 46\n",
      "2023-02-19: Hybrid: 63\n",
      "2023-02-26: Hybrid: 54\n",
      "2023-03-05: Mountain: 49\n",
      "2023-03-12: Hybrid: 53\n",
      "2023-03-19: Road: 52\n",
      "2023-03-26: Hybrid: 63\n",
      "2023-04-02: Road: 44\n",
      "2023-04-09: Hybrid: 55\n",
      "\n",
      "Shape of predictions_original_scale: (29, 3)\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform_predictions(column_transformer, X_test, predictions, dates):\n",
    "    # Get the OneHotEncoder and StandardScaler from the ColumnTransformer\n",
    "    ohe = column_transformer.named_transformers_['ohe']\n",
    "    scaler = column_transformer.named_transformers_['scaler']\n",
    "    \n",
    "    # Inverse transform the one-hot encoded product\n",
    "    product_inverse = ohe.inverse_transform(X_test[:, -1, :3])\n",
    "    \n",
    "    # Inverse transform the scaled quantities\n",
    "    quantities_inverse = predictions * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    # Combine the results\n",
    "    result = []\n",
    "    for date, prods, quants in zip(dates, product_inverse, quantities_inverse):\n",
    "        result.extend((date, prod, quant) for prod, quant in zip(prods, quants))\n",
    "    \n",
    "    return np.array(result, dtype=object)\n",
    "\n",
    "# Assuming you have a dates array corresponding to your predictions\n",
    "# If not, you'll need to create this based on your data\n",
    "dates = pd.date_range(start='2023-01-01', periods=len(predictions), freq='W')\n",
    "\n",
    "# Use the updated function\n",
    "predictions_original_scale = inverse_transform_predictions(column_transformer, X_test, predictions, dates)\n",
    "\n",
    "print(\"Sample predictions (date, product, quantity):\")\n",
    "for date, product, quantity in predictions_original_scale[:15]:\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: {product}: {float(quantity):.0f}\")\n",
    "\n",
    "# Additional debugging information\n",
    "print(\"\\nShape of predictions_original_scale:\", predictions_original_scale.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
